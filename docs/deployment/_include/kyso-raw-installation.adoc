= Kyso raw installation
:author: Kyso Inc.
:data-uri:
:doctype: article
:icons: font
:lang: en
ifndef::backend-pdf[:nofooter:] // disable footer except for PDF
:source-highlighter: rouge
:title-page:
:toc: left
:toclevels: 4
// User defined attributes
:sourcedir: ../../../..
:docsdir: ../..
:includedir: _include
:imagesdir: {docsdir}/images/_include/kyso-raw-installation
// define 'file_includes' to check it on the included files
:file_includes: true
// Include revnumber.txt (contains a macro with the kitt version number)
include::{docsdir}/revnumber.txt[]


This documentation describes how to install Kyso without using **kitt**. This docs are intended for those customers in which
internal procedures don't allow the execution of third party non-approved scripts, and therefore, are not elegible to use **kitt**.

In any case, we strongly recommend to use **kitt**, because there are lots of configurations that are managed 
automatically by kitt. Most of those configurations are centralized in the collection KysoSettings in MongoDB 
(in which we have around 60 properties), and others are in environment variables directly in the kubernetes 
deployments. For example, when you install mongodb and kyso-api using kitt, the connection string of the 
database is injected automatically in kyso-api. That kind of magic disappears without kitt, and must be 
managed manually. 

This instructions tries to be as explanatory as possible, but we strongly recommend to have an expert 
kubernetes engineer

Having said this, please follow the next instructions to install Kyso directly with helm and kubectl 

. Login into *registry.kyso.io* with the provided credentials, running the following command
+
[source,console]
----
$ docker login registry.kyso.io
----

== Install *MongoDB*

. Create the mongodb namespace

+
[source,console]
----
$ kubectl create namespace mongodb-prod
----

. In kitt folder (there are the helm charts too, we are not going to execute nothing 
with kitt, but the definitions still there), open the folder `lib/kitt/tmpl/apps/mongodb`

. That folder contains the definitions of the persistent volumes (pv.yaml), persistent volume claims (pvc.yaml)
and the values that would be used by the helm chart (values.yaml).

. Copy those 3 files into a temporary directory

. Open pvc.yaml and change the values to fit into your infrastructure. The next example defines a Persistent
Volume Claim of 8Gb in local file system using GP3 at AWS

+
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: datadir-kyso-mongodb-0
  namespace: mongodb-prod
  labels:
    app.kubernetes.io/name: mongodb
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 8Gi
  storageClassName: "gp3"    
----

+
[NOTE]
======
There are LOTS of PVC types. Please adjust the configuration of them following the provider instructions 
======

. Apply the pvc.yaml running the following command

+
[source,console]
----
kubectl apply -f pvc.yaml
persistentvolumeclaim/datadir-kyso-mongodb-0 created
----

. Open values.yaml and change the values to fit into your infrastructure. The following example 
is related to the previous PVC. Please note that this is a **standard mongodb helm chart**, for changes
checkout out its documentation.

+
[source,yaml]
----
global:
    imageRegistry: docker.io
    # If needed credentials to download the image 
    imagePullSecrets:
        - dockerconfigjson
image:
    repository: bitnami/mongodb
    # tag: not set, we expect the one from the chart to be available
architecture: replicaset
# Number of replicas for replicaset mode
replicaCount: 1
# Use statefulSet if running in standalone mode
useStatefulSet: true
arbiter:
    enabled: false
metrics:
    enabled: true
    image:
        repository: bitnami/mongodb-exporter
        # tag: not set, we expect the one from the chart to be available
    extraFlags: --compatible-mode
    # Added --compatible-mode for the metrics exporter (almost all grafana
    # dashboards available use the old variable names) and use the chart to
    # generate the serviceMonitor (we have added the release label to the
    # additionalLabels to make the prometheus operator select the
    # serviceMonitor automatically (it probably can be done using a different
    # strategy, but this is simple and works).
    serviceMonitor:
        enabled: true
        additionalLabels:
            release: null
auth:
    replicaSetKey: PVcSMXikxa
    rootPassword: 4irK7lqwTE
    databases:
        - kyso
    usernames:
        - kysodb
    passwords:
        - Ugui1dpMKP
persistence:
    enabled: true
    # Make sure the storage class matches with the previously defined PVC
    storageClass: gp3 
----

. Apply helm chart executing this

+
[source,console]
----
helm upgrade --install -n mongodb-prod -f values.yaml kyso-mongodb bitnami/mongodb --version=12.1.31
----

. Check that the deployment is running executing

+
[source,console]
----
$ kubectl get pods -n mongodb-prod
NAME             READY   STATUS    RESTARTS   AGE
kyso-mongodb-0   1/1     Running   0          76s
----

== Install *MongoGUI* (optional)

MongoGUI is an open source MongoDB explorer that could be useful, specially to change settings without 
the need of accessing the pod via shell. If you are proficient with MongoDB and shell, this step is not 
required

. Create the mongo-gui namespace

+
[source,console]
----
$ kubectl create namespace mongo-gui-prod
----

. In kitt folder (there are the helm charts too, we are not going to execute nothing 
with kitt, but the definitions still there), open the folder `lib/kitt/tmpl/apps/mongo-gui`

. That folder contains the definitions of the values that would be used by the helm chart (values.yaml).

. Copy that file into a temporary directory

. Open values.yaml and change the values to fit into your infrastructure. The following example 
is related to the previous MongoDB deployment, remember to change the values of the passwords and routes 
to fit into your configuration. Please note that this is a **standard mongo-gui helm chart**, for changes
checkout out its documentation.

+
[source,yaml]
----
replicaCount: 
image:
  repository: "registry.kyso.io/docker/mongo-gui"
  tag: "1.0.0"
  pullPolicy: "Always"
imagePullSecrets:
  - name: "dockerconfigjson"
# Service settings
service:
  targetPort: "4321"
# Ingress configuration
ingress:
  enabled: true
  className: nginx
  annotations:
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
  # PUT HERE THE MAIN DOMAIN YOU WILL USE 
  hosts:
  - host: "lo.kyso.io"
ingressAuth:
  user: "mongo-admin"
  pass: ""
# Kyso configuration file values
secrets:
  mongodbDatabaseUri: "mongodb://root:4irK7lqwTE@kyso-mongodb-0.kyso-mongodb-headless.mongodb-prod.svc.cluster.local/admin"
----

. Apply helm chart executing this

+
[source,console]
----
helm upgrade --install -n mongo-gui-prod -f values.yaml mongo-gui $KITT_HOME/lib/kitt/charts/mongo-gui
----

. Check that the deployment is running executing

+
[source,console]
----
$ kubectl get pods -n mongo-gui-prod
NAME                         READY   STATUS    RESTARTS   AGE
mongo-gui-694754b6bc-znvf4   1/1     Running   0          25s
----

. [Optional] For a quick test, lets portforward the mongo-gui deployment to check that the UI is accessible.
For that, run the following command, changing the namespace and pod name for yours

+
[source,console]
----
$ kubectl port-forward -n mongo-gui-prod mongo-gui-694754b6bc-znvf4 40000:4321
Forwarding from 127.0.0.1:40000 -> 4321
Handling connection for 40000
Handling connection for 40000
----

. Now the mongo-gui application is redirected to your local port 40000. Open the browser and access
http://localhost:40000/mongo-gui, you should see the following:
+
image::portforwarded-mongo-gui.png[Port Forwarded Mongo GUI]

+
[NOTE]
======
Use Ctrl+C to stop the portforwarding
======

== Install **nats**

. Create the nats namespace

+
[source,console]
----
$ kubectl create namespace nats-prod
----

. In kitt folder open the folder `lib/kitt/tmpl/apps/nats`

. That folder contains the definitions of the persistent volumes (pv.yaml), persistent volume claims (pvc.yaml)
and the values that would be used by the helm chart (values.yaml).

. Copy those 3 files into a temporary directory

. Open pvc.yaml and change the values to fit into your infrastructure. The next example defines a Persistent
Volume Claim of 10Gb in local file system using GP3 at AWS

+
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kyso-nats-js-pvc-kyso-nats-0
  namespace: nats-prod
  labels:
    app.kubernetes.io/name: nats
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: "gp3"
----

+
[NOTE]
======
There are LOTS of PVC types. Please adjust the configuration of them following the provider instructions 
======

. Apply the pvc.yaml running the following command

+
[source,console]
----
kubectl apply -f pvc.yaml
persistentvolumeclaim/kyso-nats-js-pvc-kyso-nats-0 created
----

. Open values.yaml and change the values to fit into your infrastructure. The following example 
is related to the previous PVC. Please note that this is a **standard nats helm chart**, for changes
checkout out its documentation.

+
[source,yaml]
----
imagePullSecrets:
  - name: "dockerconfigjson"
nats:
  image: nats:2.8.2-alpine
  jetstream:
    enabled: true
    memStorage:
      enabled: true
      size: "2Gi"
    fileStorage:
      enabled: true
      size: "10Gi"
natsbox:
  image: natsio/nats-box:0.11.0
  imagePullSecrets:
    - name: "dockerconfigjson"
reloader:
  image: natsio/nats-server-config-reloader:0.7.0
  imagePullSecrets:
    - name: "dockerconfigjson"
exporter:
  image: natsio/prometheus-nats-exporter:0.9.3
  imagePullSecrets:
    - name: "dockerconfigjson"
cluster:
  enabled: false
  name: "kyso-nats"
  replicas: 1
----

. Apply helm chart executing this

+
[source,console]
----
helm upgrade --install -n nats-prod -f values.yaml kyso-nats nats/nats --version=0.17.0
----

. Check that the deployment is running executing

+
[source,console]
----
$ kubectl get pods -n nats-prod
NAME                            READY   STATUS    RESTARTS   AGE
kyso-nats-box-9cd6697db-zt6td   1/1     Running   0          17s
kyso-nats-0                     3/3     Running   0          17s
----

== Install **Elasticsearch**

. Create the elasticsearch namespace

+
[source,console]
----
$ kubectl create namespace elasticsearch-prod
----

. In kitt folder open the folder `lib/kitt/tmpl/apps/elasticsearch`

. That folder contains the definitions of the persistent volumes (pv.yaml), persistent volume claims (pvc.yaml)
and the values that would be used by the helm chart (values.yaml).

. Copy those 3 files into a temporary directory

. Open pvc.yaml and change the values to fit into your infrastructure. The next example defines a Persistent
Volume Claim of 30Gb in local file system using GP3 at AWS

+
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: elasticsearch-master-elasticsearch-master-0
  namespace: elasticsearch-prod
  labels:
    app.kubernetes.io/name: elasticsearch
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 30Gi
  storageClassName: "gp3"
----

+
[NOTE]
======
There are LOTS of PVC types. Please adjust the configuration of them following the provider instructions 
======

. Apply the pvc.yaml running the following command

+
[source,console]
----
kubectl apply -f pvc.yaml
persistentvolumeclaim/elasticsearch-master-elasticsearch-master-0 created
----

. Open values.yaml and change the values to fit into your infrastructure. The following example 
is related to the previous PVC. Please note that this is a **standard elasticsearch helm chart**, for changes
checkout out its documentation.

+
[source,yaml]
----
replicas: 1
image: docker.elastic.co/elasticsearch/elasticsearch
# imageTag: not set, we expect the one from the chart to be available
# To make pods spread between zones could change the antiAffinityTopologyKey to
# 'topology.kubernetes.io/zone', but for now that is OK and while we have one
# node per zone all will work as expected.
antiAffinityTopologyKey: kubernetes.io/hostname
# set to hard, we will need as many nodes as replicas
antiAffinity: hard
esJavaOpts: ""
resources:
    requests:
        cpu: 1000m
        memory: 2Gi
clusterHealthCheckParams: local=true
volumeClaimTemplate:
    accessModes:
        - ReadWriteOnce
    resources:
        requests:
            storage: 30Gi
    storageClassName: gp3
----

. Apply helm chart executing this

+
[source,console]
----
helm upgrade --install -n elasticsearch-prod -f values.yaml kyso-elasticsearch elastic/elasticsearch --version=7.17.3
----

. Check that the deployment is running executing

+
[source,console]
----
$ kubectl get pods -n elasticsearch-prod 
NAME                     READY   STATUS    RESTARTS   AGE
elasticsearch-master-0   1/1     Running   0          26s
----

. [Optional] Let's check quickly that the Elasticsearch is effectively running. You can use any 
elasticsearch client to test it, but if you don't have any we recommend you https://elasticvue.com/[ElasticVue]. Before connecting to Elastic, we need to perform a port forwarding to your local port executing the following command (please adapt the command to your namespace and pod naming)

+
[source,console]
----
kubectl port-forward -n elasticsearch-prod elasticsearch-master-0 40001:9200
----

. Now, open your Elasticsearch client to configure the connection. For this example we are going to use ElasticVue as commented before. **Elasticsearch is configured without any username and password, because is NOT ACCESIBLE OUTSIDE THE INSTALLATION**. This is the connection configuration we used

+
image::elastic-0.png[Connection to Elasticsearch]

+
image::elastic-1.png[Information of nodes]

+
[NOTE]
======
Use Ctrl+C to stop the portforwarding
======

== Install **kyso-api**

. Create the kyso-api namespace

+
[source,console]
----
$ kubectl create namespace kyso-api-prod
----

. In kitt folder, open the folder `lib/kitt/tmpl/apps/kyso-api`

. That folder contains the definitions of the values that would be used by the helm chart (values.yaml) and the definition of the SVC (svc_map.yaml)

. Copy these files into a temporary directory

. Open svc_map.yaml and change the values to fit into your infrastructure. The following example is related to previous deployments, and future deployments. Remember to change the values to fit into your configuration. This configuration allows kyso-api to communicate with other services

+
[source,yaml]
----
# The first service will be removed, it is used only while moving from kitt 1.x
# to 2.x (it is needed if kyso-api is updated before kyso-scs, as we have
# changed the service name removing the '-svc' suffix)
apiVersion: v1
kind: Service
metadata:
  name: kyso-api-svc
  namespace: kyso-api-prod
spec:
  type: ExternalName
  externalName: kyso-api
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: kyso-api-prod
spec:
  type: ExternalName
  externalName: elasticsearch-master.elasticsearch-prod.svc.cluster.local
  ports:
  - name: elastisearch
    port: 9200
---
apiVersion: v1
kind: Service
metadata:
  name: kyso-nbdime
  namespace: kyso-api-prod
spec:
  type: ExternalName
  externalName: kyso-nbdime.kyso-nbdime-prod.svc.cluster.local
  ports:
  - name: http
    port: 80
---
apiVersion: v1
kind: Service
metadata:
  name: kyso-scs
  namespace: kyso-api-prod
spec:
  type: ExternalName
  externalName: kyso-scs.kyso-scs-prod.svc.cluster.local
  ports:
  - name: sftp
    port: 22
  - name: indexer
    port: 8080
  - name: webhook
    port: 9000
---
apiVersion: v1
kind: Service
metadata:
  name: nats
  namespace: kyso-api-prod
spec:
  type: ExternalName
  externalName: kyso-nats.nats-prod.svc.cluster.local
  ports:
  - name: nats
    port: 4222
---
apiVersion: v1
kind: Service
metadata:
  name: mongodb
  namespace: kyso-api-prod
spec:
  type: ExternalName
  externalName: kyso-mongodb-0.kyso-mongodb-headless.mongodb-prod.svc.cluster.local
  ports:
  - name: mongodb
    port: 27017
----

. Apply svc_map.yaml executing the following instruction

+
[source,yaml]
----
$ kubectl apply -f svc_map.yaml
service/kyso-api-svc created
service/elasticsearch created
service/kyso-nbdime created
service/kyso-scs created
service/nats created
service/mongodb created
----

. Open values.yaml and change the values to fit into your infrastructure. The following example 
is related to the previous MongoDB deployment, remember to change the values of the passwords and routes 
to fit into your configuration.

+
[source,yaml]
----
replicaCount: 1
image:
    # Put here the repository in which the images are placed
    repository: registry.kyso.io/kyso-io/kyso-api
    # Specify the desired tag
    tag: 2.0.20
    pullPolicy: IfNotPresent
imagePullSecrets:
    - name: dockerconfigjson
# Endpoint settings
endpoint:
    enabled: false
    ip: ""
    port: "4000"
# Service settings
service:
    targetPort: "4000"
# Ingress configuration
ingress:
    enabled: true
    annotations:
        nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
        nginx.ingress.kubernetes.io/proxy-body-size: 500m
    hosts:
        # Specify the host in which kyso will respond on
        - host: lo.kyso.io
# Ingress API Docs configuration (optional)
ingressDocs:
    enabled: false
    annotations:
        nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
# Basic authentication if required (optional)
ingressDocsAuth:
    user: apidoc
    pass: ""
# Kyso configuration file values
envSecret:
    # Connection String to MongoDB
    mongodbDatabaseUri: mongodb://kysodb:Ugui1dpMKP@kyso-mongodb-0.kyso-mongodb-headless.mongodb-prod.svc.cluster.local/kyso
    # To populate Kyso with testing metadata (optional)
    populateMailPrefix: lo
    populateTestData: true

----

. Apply helm chart executing this

+
[source,console]
----
helm upgrade --install -n kyso-api-prod -f values.yaml kyso-api $KITT_HOME/lib/kitt/charts/kyso-api
----

. Check that the deployment is running executing

+
[source,console]
----
$ kubectl get pods -n kyso-api-prod
NAME                        READY   STATUS    RESTARTS   AGE
kyso-api-746575c79f-fkh2x   1/1     Running   0          40s
----

. [Optional] For a quick test, lets portforward the kyso-api deployment to check that it is effectively running. For that, run the following command, changing the namespace and pod name for yours

+
[source,console]
----
$ kubectl port-forward -n kyso-api-prod kyso-api-746575c79f-fkh2x 40002:4000
Forwarding from 127.0.0.1:40002 -> 4000
Handling connection for 40002
Handling connection for 40002
----

+
[NOTE]
======
Use Ctrl+C to stop the portforwarding
======

. Now kyso-api is redirected to your local port 40002. Open the browser and access
http://localhost:40002/api/v.html, you should see the following:

+
image::kyso-api-0.png[Port Forwarded Kyso API]

. If you still have the mongo-gui port forwarded, access to http://localhost:40000/mongo-gui. You will see a new database named **kyso**, with several collections. The most interesting collection is KysoSettings, in which there are important configuration settings.

+
image::kyso-api-1.png[Populated database]

. Adjust the following variables at KysoSettings collection with the values according to your deployment

[cols="1,1,1"]
|===
|**key**
|**description**
|**example value**

|BASE_URL
|domain in which kyso will be served
|https://lo.kyso.io

|MAIL_TRANSPORT
|SMTP mail transport using the following format: smtps://USER:PASSWORD@MAIL.DOMAIN
|smtps://dev@dev.kyso.io:thepassword@mailu.kyso.io

|MAIL_FROM
|Sender name and email which will send emails in name of Kyso
|"kyso" <dev@dev.kyso.io>

|FRONTEND_URL
|domain in which kyso will be served (same as BASE_URL, legacy setting)
|https://lo.kyso.io

|ELASTICSEARCH_URL
|Internal Kubernetes URL for Elasticsearch. **DEPENDS ON ELASTICSEARCH DEPLOYMENT**
|http://**elasticsearch-master**.**elasticsearch-prod**.svc.cluster.local:**9200**

|KYSO_NATS_URL
|Internal Kubernetes URL for NATS. **DEPENDS ON NATS DEPLOYMENT**
|nats://**kyso-nats**.**nats-prod**.svc.cluster.local:**4222**

|=== 

== Install **kyso-front**

. Create the kyso-front namespace

+
[source,console]
----
$ kubectl create namespace kyso-front-prod
----

. In kitt folder, open the folder `lib/kitt/tmpl/apps/kyso-front`

. That folder contains the definitions of the values that would be used by the helm chart (values.yaml) and the definition of the SVC (svc_map.yaml)

. Copy these files into a temporary directory

. Open svc_map.yaml and change the values to fit into your infrastructure. The following example is related to previous deployments, and future deployments. Remember to change the values to fit into your configuration. This configuration allows kyso-api to communicate with other services

+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: kyso-front-prod
spec:
  type: ExternalName
  externalName: elasticsearch-master.elasticsearch-prod.svc.cluster.local
  ports:
  - name: elastisearch
    port: 9200
---
apiVersion: v1
kind: Service
metadata:
  name: kyso-scs
  namespace: kyso-front-prod
spec:
  type: ExternalName
  externalName: kyso-scs.kyso-scs-prod.svc.cluster.local
  ports:
  - name: sftp
    port: 22
  - name: indexer
    port: 8080
  - name: webhook
    port: 9000
---
apiVersion: v1
kind: Service
metadata:
  name: nats
  namespace: kyso-front-prod
spec:
  type: ExternalName
  externalName: kyso-nats.nats-prod.svc.cluster.local
  ports:
  - name: nats
    port: 4222
---
apiVersion: v1
kind: Service
metadata:
  name: mongodb
  namespace: kyso-front-prod
spec:
  type: ExternalName
  externalName: kyso-mongodb-0.kyso-mongodb-headless.mongodb-prod.svc.cluster.local
  ports:
  - name: mongodb
    port: 27017
----

. Apply svc_map.yaml executing the following instruction

+
[source,yaml]
----
$ kubectl apply -f svc_map.yaml
service/elasticsearch created
service/kyso-scs created
service/nats created
service/mongodb created
----

. Open values.yaml and change the values to fit into your infrastructure. The following example 
is related to the previous MongoDB deployment, remember to change the values of the passwords and routes 
to fit into your configuration.

+
[source,yaml]
----
replicaCount: 1
image:
    repository: registry.kyso.io/kyso-io/kyso-front
    tag: 2.0.20
    pullPolicy: IfNotPresent
imagePullSecrets:
    - name: dockerconfigjson
# Endpoint settings
endpoint:
    enabled: false
    ip: ""
    port: "3000"
# Service settings
service:
    targetPort: "3000"
# Ingress configuration
ingress:
    enabled: true
    annotations:
        nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    hosts:
        - host: lo.kyso.io
----

. Apply helm chart executing this

+
[source,console]
----
helm upgrade --install -n kyso-front-prod -f values.yaml kyso-front $KITT_HOME/lib/kitt/charts/kyso-front
----

. Check that the deployment is running executing

+
[source,console]
----
$ kubectl get pods -n kyso-front-prod
NAME                         READY   STATUS    RESTARTS   AGE
kyso-front-b4d6fc8cf-pgrtk   1/1     Running   0          3m14s
----

. [Optional] For a quick test, lets portforward the kyso-api deployment to check that it is effectively running. For that, run the following command, changing the namespace and pod name for yours

+
[source,console]
----
$ kubectl port-forward -n kyso-front-prod kyso-front-b4d6fc8cf-pgrtk 40003:3000
Forwarding from 127.0.0.1:40003 -> 3000
Handling connection for 40003
Handling connection for 40003
----

+
[NOTE]
======
Use Ctrl+C to stop the portforwarding
======

. Now kyso-front is redirected to your local port 40003. Open the browser and access
http://localhost:40003/login, you should see the following:

+
image::kyso-front-0.png[Port Forwarded Kyso front]

+
[IMPORTANT]
======
kyso-front expects the API to be available in the same domain, in the subroute **/api**. That's not happening now, and in consequence, the login is not going to work. This will be solved later when we install the ingress routes.
======

== Install **kyso-scs**

. Create kyso-scs namespace

+
[source,console]
----
$ kubectl create namespace kyso-scs-prod
----

. In kitt folder open the folder `lib/kitt/tmpl/apps/kyso-scs`

. That folder contains the definitions of the persistent volumes (pv.yaml), persistent volume claims (pvc.yaml), services (svc_map.yaml) and values that would be used by the helm chart (values.yaml).

. Copy those 4 files into a temporary directory

. Open pvc.yaml and change the values to fit into your infrastructure. The next example defines a Persistent Volume Claim of 10Gb in local file system using **EFS** at AWS. Please adjust these parameters
into your infrastructure.

+
[WARNING]
======
kyso-scs is the most storage consuming component in kyso's architecture. Here is where the files of the reports are stored (version by version), and other public data like portraits, images, themes, etc. In a production environment we recommend to use a scalable filesystem like **AWS EFS**, but you can use a regular volume enough big to store your reports (you can redimension it when its necessary)
======

+
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kyso-scs-prod
  namespace: kyso-scs-prod
  labels:
    app.kubernetes.io/name: kyso-scs
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 8Gi
  # If no storageClassName is defined, kubernetes will take the default (if it's defined)
  #storageClassName: "gp3"
  storageClassName: "efs-sc"
----

+
[NOTE]
======
There are LOTS of PVC types. Please adjust the configuration of them following the provider instructions 
======

. Apply the pvc.yaml running the following command

+
[source,console]
----
kubectl apply -f pvc.yaml
persistentvolumeclaim/kyso-scs-prod created
----

. Create a new file named `configmap.yaml` with the next contents, but adjusted to your infrastructure.

+
[source,yaml]
----
apiVersion: v1
data:
  application.yaml: |
    cron:
      # Frequency in which the cronjobs will be executed
      expr: "*/30 * * * * ?"
    app:
      indexer:
        # Elasticsearch kubernetes internal name
        elasticsearch: "http://elasticsearch-master.elasticsearch-prod.svc.cluster.local:9200"
        # Location of temporary files
        filepath: "/tmp"
        # Base location of scs data (where the report files will be placed)
        scsBasePath: "/sftp/data/scs"
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: kyso-indexer-config
  namespace: kyso-scs-prod
----

. Apply `configmap.yaml` running the following command

+
[source,console]
----
kubectl apply -f configmap.yaml
configmap/kyso-indexer-config created
----

. Open `svc_map.yaml` and adjust the parameters to the adequate values of your infrastructure. The following example has the values related to the previous deployments

+
[source,yaml]
----
# The first service will be removed, it is used only while moving from kitt 1.x
# to 2.x (it is needed if kyso-scs is updated before kyso-api, as we have
# changed the service name removing the '-svc' suffix)
apiVersion: v1
kind: Service
metadata:
  name: kyso-scs-svc
  namespace: kyso-scs-prod
spec:
  type: ExternalName
  externalName: kyso-scs
  ports:
  - name: sftp
    port: 22
  - name: indexer
    port: 8080
  - name: webhook
    port: 9000
---
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: kyso-scs-prod
spec:
  type: ExternalName
  externalName: elasticsearch-master.elasticsearch-prod.svc.cluster.local
  ports:
  - name: elastisearch
    port: 9200
---
apiVersion: v1
kind: Service
metadata:
  name: nats
  namespace: kyso-scs-prod
spec:
  type: ExternalName
  externalName: kyso-nats.nats-prod.svc.cluster.local
  ports:
  - name: nats
    port: 4222
---
apiVersion: v1
kind: Service
metadata:
  name: mongodb
  namespace: kyso-scs-prod
spec:
  type: ExternalName
  externalName: kyso-mongodb-0.kyso-mongodb-headless.mongodb-prod.svc.cluster.local
  ports:
  - name: mongodb
    port: 27017
----


. Apply `svc_map.yaml` running the following command

+
[source,console]
----
kubectl apply -f svc_map.yaml
configmap/kyso-indexer-config created
----

. Open `values.yaml`` and change the values to fit into your infrastructure. The following example 
is related to the previous deployments.


+
[source,yaml]
----
replicaCount: 1
dataPvcName: kyso-scs-prod
dataVolName: kyso-scs-prod
sftpPubUser: pub
sftpScsUser: scs
cronjobs:
    hardlink:
        enabled: true
        image:
            repository: registry.kyso.io/docker/alpine
            pullPolicy: IfNotPresent
            tag: latest
        schedule: 0 0 * * *
        webhookUrl: http://kyso-scs-svc.kyso-scs-prod.svc.cluster.local:9000/hooks/hardlink
containers:
    indexer:
        endpoint:
            # if the endpoint is enabled we use it instead of the image
            enabled: false
            ip: ""
            port: 8080
        config:
            # The following values are used by the application.yaml config file
            cronExpr: '*/30 * * * * ?'
            elasticsearchUrl: http://elasticsearch:9200
            mongodbDatabaseUri: mongodb://kysodb:Ugui1dpMKP@kyso-mongodb-0.kyso-mongodb-headless.mongodb-prod.svc.cluster.local/kyso
        image:
            repository: registry.kyso.io/kyso-io/kyso-indexer
            pullPolicy: IfNotPresent
            tag: 2.3.3
        service:
            port: 8080
            targetPort: 8080
    myssh:
        image:
            repository: registry.kyso.io/docker/mysecureshell
            pullPolicy: IfNotPresent
            tag: 2.0.0
        secretName: kyso-scs-myssh-secret
        service:
            port: 22
            targetPort: 22
    nginx:
        image:
            repository: registry.kyso.io/docker/nginx-scs
            pullPolicy: IfNotPresent
            tag: 3.0.0
        service:
            port: 80
            targetPort: 80
        env:
            - name: AUTH_REQUEST_URI
              value: http://kyso-api.kyso-api-prod.svc.cluster.local/api/v1/auth/check-permissions
    webhook:
        enabled: true
        image:
            repository: registry.kyso.io/docker/webhook-scs
            pullPolicy: IfNotPresent
            tag: 1.2.5
        service:
            port: 9000
            targetPort: 9000
        env:
            - name: KYSO_URL
              value: http://kyso-api.kyso-api-prod.svc.cluster.local
imagePullSecrets:
    - name: dockerconfigjson
ingress:
    enabled: true
    annotations:
        nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    hosts:
        - host: lo.kyso.io
podAnnotations:
    # Add annotations for velero
    backup.velero.io/backup-volumes: kyso-scs-prod
----

. Apply helm chart executing this

+
[source,console]
----
helm upgrade --install -n kyso-scs-prod -f values.yaml kyso-scs $KITT_HOME/lib/kitt/charts/kyso-scs
----

. Check that the deployment is running executing

+
[source,console]
----
$ kubectl get pods -n kyso-scs-prod
NAME         READY   STATUS    RESTARTS   AGE
kyso-scs-0   4/4     Running   0          6m10s
----

. Obtain the credentials for kyso-scs executing the following command

+
[NOTE]
======
We are using https://github.com/jqlang/jq/wiki/Installation[jq] command to process json easily.

jq is available in all operating systems
======

+
[source,console]
----
$ kubectl get secret -n kyso-scs-prod kyso-scs-myssh-secret -o json | jq -r '.data."user_pass.txt"' | base64 -d
pub:WxIqIIa8I5BBtOnp
scs:8LQYSUh1Dyj98l9o
----

. Adjust the following variables at KysoSettings collection with the values according to your deployment

[cols="1,1,1"]
|===
|**key**
|**description**
|**example value**

|SFTP_HOST
|kyso-scs internal kubernetes name
|kyso-scs.kyso-scs-prod.svc.cluster.local

|SFTP_HOST
|kyso-scs port
|22

|MAIL_FROM
|Sender name and email which will send emails in name of Kyso
|"kyso" <dev@dev.kyso.io>

|SFTP_USERNAME
|kyso-scs username
|scs

|SFTP_PASSWORD
|Password for user scs obtained previously. **Adjust with your values**
|8LQYSUh1Dyj98l9o

|SFTP_PUBLIC_USERNAME
|kyso-scs user for public files
|pub

|SFTP_PUBLIC_PASSWORD
|Password for public user obtained previously. **Adjust with your values**
|WxIqIIa8I5BBtOnp

|STATIC_CONTENT_PREFIX
|
|/scs

|STATIC_CONTENT_PUBLIC_PREFIX
|
|/pub

|REPORT_PATH
|Path where reports are unzipped
|/data

|TMP_FOLDER_PATH
|Path where temporary files are stored
|/app/data

|KYSO_INDEXER_API_BASE_URL
|Base URL of the kyso indexing service
|http://kyso-scs.kyso-scs-prod.svc.cluster.local:8080

|KYSO_WEBHOOK_URL
|Webhooks URL (s3 import, du, etc.)
|http://kyso-scs.kyso-scs-prod.svc.cluster.local:9000
|=== 

== Install **ingress**

Now that we have the essential parts of Kyso, we need to make them accessible without the need of any portforwarding. 

=== Certificates

Kyso requires **https**. In this section we are going to prepare the configuration for SSL Certificates. These are the prerrequisites necessary

[cols="1"]
|===
|**prerequisite**

|Certificate KEY file

|Certificate CRT file

|http://kyso-scs.kyso-scs-prod.svc.cluster.local:9000
|=== 

. Create a namespace `ingress`

+
[source,console]
----
$ kubectl create namespace ingress
----

. Create a new secret with the desired certificates

+
[source,console]
----
kubectl create secret tls ingress-cert \
    --namespace ingress \
    --cert /route/to/your/domain.crt \
    --key /route/to/your/domain.key
----

+
[NOTE]
======
Put in tls.crt the content of your certificate crt file

Put in tls.key the content of your certificate key file
======

. Test that the secret is effectively created running

+
[source,console]
----
$ kubectl get secret -n ingress
NAME           TYPE                DATA   AGE
ingress-cert   kubernetes.io/tls   2      2d17h
----

=== nginx ingress

. Now, we are going to install the ingress's helm chart. First of all, copy the file placed at your kitt folder `lib/kitt/tmpl/addons/ingress`. There are two files, `values.yaml` and `coredns-custom.yaml`

. Substitute the properties at `values.yaml` with the adequate to your deployment. The following example represents a generic configuration

+
[source,yaml]
----
config:
  enable-brotli: "true"
  use-forwarded-headers: "true"
  compute-full-forwarded-for: "true"
  enable-real-ip: "true"
  use-gzip: "true"
image:
  registry: "docker.io"
  repository: "bitnami/nginx-ingress-controller"
  tag: "1.5.1-debian-11-r5"
defaultBackend:
  image:
    registry: "docker.io"
    repository: "bitnami/nginx"
    tag: "1.22.1-debian-11-r7"
ingressClassResource:
  default: true
extraArgs:
  # Make sure this property matches with previosly defined secret name
  default-ssl-certificate: "ingress/ingress-cert"
  ingress-class: nginx
replicaCount: 1
----

. Apply the helm chart executing the following command

+
[source,console]
----
helm upgrade --install -n ingress -f values.yaml ingress bitnami/nginx-ingress-controller --version=9.3.22
----

. Check that everything is running executing the following command

+
[source,console]
----
$ kubectl get pods -n ingress
NAME                                                              READY   STATUS    RESTARTS   AGE
ingress-nginx-ingress-controller-default-backend-f687c95978ln9h   1/1     Running   0          42s
ingress-nginx-ingress-controller-644bf9db5c-mpksj                 1/1     Running   0          42s
----

. Let's check that the ingress is effectively running, checking that the it's answering to the defined domain name, and applying the certificates. Open your browser and open the following url https://<your_domain>/api/v.html. You must see something similar to the following screenshot (we used the domain https://lo.kyso.io/api/v.html)

+
image::ingress-0.png[API being served by ingress]

. If you access directly to https://<your_domain>/login, you should see something similar to the following screenshot (we used the domain https://lo.kyso.io)

+
image::ingress-1.png[Kyso being served by ingress]

+
[NOTE]
======
Note that the certificates are valid!
======

+
image::ingress-2.png[Kyso being served by ingress]

. You can check all the ingresses running in your kubernetes cluster running the next command

+
[source,console]
----
$ kubectl get ingress -A
NAMESPACE         NAME         CLASS   HOSTS        ADDRESS      PORTS   AGE
kyso-scs-prod     kyso-scs     nginx   lo.kyso.io   172.22.0.2   80      3d3h
kyso-api-prod     kyso-api     nginx   lo.kyso.io   172.22.0.2   80      3d23h
mongo-gui-prod    mongo-gui    nginx   lo.kyso.io   172.22.0.2   80      4d2h
kyso-front-prod   kyso-front   nginx   lo.kyso.io   172.22.0.2   80      3d22h
----

